\section{The Compiler}
\label{sec:compiler}


In this section we present our compiler. We start by sketching its architecture.

The compiler accepts an SQL query and schema definition (essentially create table statements for the relations the query accesses) as input. The parser turns the input query into a first intermediate representation (IR) that is somewhat reminiscent of relational algebra. The first compilation stage generates trigger programs for performing multilevel incremental view maintenance (IVM). The statements of these trigger programs increment the values of view data structures (called maps) by expressions of this IR. The IR and basics of the transformation are described in Section~\ref{sec:compiler_calc}. Section~\ref{sec:simplification} presents optimizations for simplifying IRs in the first compilation stage.

In Section~\ref{sec:advanced-rewriting} we discuss the creation of triggers for multi-level incremental view maintenance by extracting and materializing strict subexpressions rather than the full query. This is necessary to be able to deal with nested aggregation which otherwise, if we always materialize the largest query we can, leads to recursion in compilation that does not terminate. In general, the choice of subquery to materialize and incrementally maintain is a degree of freedom in query optimization. In this section we give heuristic rules for making this choice.

The next stage of the compiler, described in Section~\ref{sec:kthree}, turns the statements of the trigger program created in the first stage into an IR for purely functional programming, essentially lambda calculus without general recursion but with special higher-order functions for transforming collections (primitives such as map and reduce). This stage allows us to perform deforestation and fusion optimizations too low-level to have expression in the first, relational algebra-like IR.

Section~\ref{sec:codegen} describes our infrastructure for code generation, which makes use of a third imperative IR, for generating imperative programs and performing final, low-level optimization before actual code is emitted. 

Section~\ref{sec:runtime} describes our runtime system including the machinery for reading update streams.

TODO: show an architecture diagram.




\subsection{Queries with binding patterns, deltas, and recursive IVM}
\label{sec:compiler_calc}


\def\Sum{\mbox{Sum}}


Next we develop an IR for SQL queries that is suitable for compilation of database queries and to perform the transformations necessary to enable efficient multilevel IVM. This language is a refinement of the query language defined in \cite{koch-pods:10}, but here we aim at better readability and avoid unnecessary formality. The language is based on positive relational algebra with aggregation but makes the following modifications:
%
\begin{itemize}
\item
The data model is that of relations where tuples have {\em integer} multiplicities. This generalizes SQL bag semantics to allow for
negative multiplicities. This way, databases and updates can be treated uniformly. A relation in which all multiplicities are
nonnegative can be either thought of as an insertion or as a database (=an insertion into the empty database) and a relation in which all tuple multiplicities are negative is a deletion.

\item
We replace relational (bag) union by an operation $+$ that adds two relations $R$ and $S$ by assigning to each tuple in the result the integer sum of the multiplicities of that tuple in $R$ and $S$. The operation $+$ is associative, and this is key to the overall simplicity of the
approach and allows us to treat insertions and deletions uniformly.

\item
The join operation is the natural join, where the multiplicity of a tuple in the result
is the integer product of the multiplicities of the two tuples from which is formed.

\item
Conditions are queries in their own right; there is no explicit selection operation. Thus, we write a relational algebra selection
$\sigma_{A<B}(R)$ as $R \bowtie (A<B)$.

\item
Sum-aggregates are of the form $\Sum_{\vec{A}} Q$ where $Q$ is a query and $\vec{A}$ is a tuple of group-by columns (which we will also
call variables). The result are the tuples of the projection of $Q$ on $\vec{A}$ and each tuple's multiplicity is the sum of the
multiplicities of the tuples that were projected down to it.

\item
There are terms as in SQL, who have a scalar rather than relation value. A sum-aggregate without grouping column evaluates to a term.

\item
Terms can be used as queries; in that case, they evaluate to the nullary tuple with multiplicity the value of the term.
Thus an SQL query ``select sum(A) from R'' can be written in our IR as $\Sum(R \bowtie A)$; thus we first multiply the value of $A$ into the multiplicity of each tuple and than sum up all these multiplicities.

\item
There are no explicit operations for projection and relational difference. A multiplicity-aware projection is implemented by
sum-aggregation and universal queries can be expressed by counting aggregation (a popular homework exercise in database courses).
\end{itemize}

Let us recap: The language departs from relational bag algebra in essentially two ways; by having integer tuple multiplicities
to deal uniformly with insertions and deletions, and by {\em taking aggregate values out of the tuple} and putting them down into
the multiplicity. This has a very desirable consequence: Incremental computation is all about modifying multiplicities.
As we will see later, by keeping aggregate values in the multiplicities, delta processing for aggregates is vastly simplified.
Aggregate queries dominate analytical workloads and can greatly profit from incremental view maintenance. Thus it makes sense to optimize our IR for aggregate processing.




In summary, the abstract syntax of the IR is two-sorted (consisting of queries $q$ and terms $t$):
%
\begin{eqnarray*}
q &\mbox{:--}& R \mid t \theta t \mid t \mid q + q \mid q \bowtie q \mid \Sum_{\vec{A}}(q) \\
t &\mbox{:--}& c \mid A                 \mid t + t \mid t * t       \mid \Sum(q)
\end{eqnarray*}
Thus queries can be formed from relation names, atomic conditions, addition, natural join, and sum-aggregation.
Terms are formed from constants, variables/columns, addition, multiplication, and sum-aggregation without group-by.
The semantics was given above.


We will often use SQL syntax for conciseness, which will correspond to our IR in the natural way analogous to how we translate
between classical bag relational algebra and SQL.

It is important to observe that in general, expressions of the IR have binding patterns: There are input variables or parameters
without which we cannot evaluate these expressions, and there are output variables, the columns of the schema of the query result.

Each expression $Q$ has input variables or parameters $\vec{x_{in}}$ and a set of output variables $\vec{x_{out}}$, which form the schema of the query result. We denote such an expression as $Q[\vec{x_{in}}][\vec{x_{out}}]$. The input variables are those that are not {\em range-restricted} in a calculus formulation, or equivalently have to be understood as {\em parameters} in a SQL query because their values cannot be computed from the database: They have to be provided so that the query can be evaluated.

Binding patterns represent information flow. In general, this flow is not exclusively bottom-up.
Some of an expression's variables are input variables or parameters which cannot be computed from the query but have to be given to the query so that it can be evaluated. The most interesting case of this is a correlated nested aggregate, viewed in isolation (which must be possible for small-step compositionality). In such an aggregate, the correlation variable from the outside is such an input variable. The aggregate query can only be computed if a value for the input variable is given.


We illustrate this by a few examples in Table~\ref{tab:ir-examples}.
All of the expressions there are valid queries of our IR, typed by indicating the input and output variables.

\begin{table}
\begin{footnotesize}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
select * from R   & $R$ & $A^f,B^f$ \\
--- & $C < D$ & $C^b,D^b$ \\
--- & $C = D$ & $C^b,D^f$ \\
&& or $D^b,C^f$
\\[1ex]
select A from R   & $\Sum_A(R \bowtie (B < C))$ & $C^b,A^f$ \\
where B < C       &&
\\[1ex]
select A, sum(1) from R  & $\Sum_A(R \bowtie (B < C))$ & $C^b,A^f$ \\
where B < C group by A &&
\\[1ex]
B < (select sum(D) from S & $B < \Sum(S \bowtie D$ & $A^b, B^b$ \\
~~where A > C) & ~~$\bowtie (A > C))$ &
\\[1ex]
select * from R where & $\Sum_{A,B}(R \; \bowtie$ & $A^f, B^f$ \\
B < (select sum(D) from S & ~~~~$B < \Sum(S \bowtie D$ & \\
where A > C) & ~~~~~~$\bowtie\; (A > C)))$ & \\
\hline
\end{tabular}
\end{center}
\end{footnotesize}
\caption{Some example queries with their binding patterns. Relation $R$ and S have schema $A,B$ and $C,D$, respectively.}
\label{tab:ir-examples}
\end{table}



\medskip



Note that this language specification covers most of the core features of SQL. It excludes
null values and outer joins (which have the hugely undesirably property of being nonassociative, which is a problem for incremental computation) and 
min/max aggregates. The latter can be supported in incremental evaluation but require additional data structures to support deletion. (We have to keep around all the tuples being aggregated to deal with the case that the min or max tuple gets deleted.) We do not further cover min and max aggregates in this paper.




\subsubsection{Computing the delta of a query.}

This language has the nice property of being closed under taking deltas. For each query expression $Q$, there
is an expression $\Delta Q$ that expresses how the result of $Q$ changes as the database $D$ is changed by update workload $\Delta D$. This works for updates containing an unbounded number of insertions and deletions, but we will subsequently only study single-tuple inserts and deletes.
The reason for this is that such updates allow for particularly efficient view refresh code that
can be run to respond online to each tuple change.

Proposition: The query language is closed under taking deltas.

Thanks to the strong compositionality of the language, we only have to give delta rules for the 
individual operators. These rules a given and studied in detail in \cite{koch-pods:10}. In short,
\begin{eqnarray*}
\Delta(Q_1 \cup Q_2)    &:=& (\Delta Q_1) \cup (\Delta Q_2), \\ 
\Delta(\mbox{Sum } Q)   &:=& \mbox{Sum } (\Delta Q), \\
\Delta(Q_1 \bowtie Q_2) &:=& ((\Delta Q_1) \bowtie Q_2) \cup (Q_1 \bowtie (\Delta Q_2)) \\
&& \cup\; ((\Delta Q_1) \bowtie (\Delta Q_2)).
\end{eqnarray*}

The deltas of conditions are 0 if they do not contain nested queries. The delta for a condition with a nested query
is more complicated. For example, our rule for $\Delta (x > Q)$ is
$(x > (Q \cup \Delta Q)) \bowtie (x \le Q) - (x \le (Q \cup \Delta Q)) \bowtie (x > Q)$.
We refer to \cite{koch-pods:10} for the general case of conditions.

TODO: explain minus.

Let us be precise though about computing delta queries. In general, each expression has input and output variables,
and taking a delta in general {\em adds variables} parameterizing the query with the update. From now on we only consider single-tuple insertions $+R(\vec{x})$ to or deletions $-R(\vec(x)$ from a relation $R$.
For example, given schema $R(AB), S(BC)$, the delta $+R(x,y)$ to query \\
{\tt sum(R.A * S.C) from R, S where R.B<S.B)[][]}
is \\
{\tt sum(x * S.C) from S where y < S.B[x,y][]}.
Various optimizations are possible in special cases. For example, the delta to
{\tt sum(R.A * S.C) from R, S where R.B=S.B)[][]} is \\
{\tt x * sum(S.C) from S)[][y=S.B]}. Here two things have happened. We have exploited distributivity
to pull $x$ out of the aggregation; moreover, the domain of $y$ is restricted by the database; the possible values of $y$ can be computed from the database (bottom-up), so it can be turned into an output variable.
To understand better why our delta expressions have this shape, note that \\
$\Delta_{+R(x,y)}(R[][A, B_R] \bowtie S[][B_S, C] \bowtie B_R \theta B_S)$ is \\
$(\Delta_{+R(x,y)}R[][A, B_R]) \bowtie (S[][B_S, C] \bowtie B_R \theta B_S))$ \\
+
$(\Delta_{+R(x,y)}R[][A, B_R]) \bowtie (\Delta_{+R(x,y)}(S[][B_S, C] \bowtie B_R \theta B_S)))$ \\
+
$R[][A, B_R] \bowtie (\Delta_{+R(x,y)}(S[][B_S, C] \bowtie B_R \theta B_S)))$ and \\
$\Delta_{+R(x,y)}(S[][B_S, C] \bowtie B_R \theta B_S)) = 0$ \\
since both
$\Delta_{+R(x,y)} S[][B_S, C]$ and
$\Delta_{+R(x,y)} B_R \theta B_S$ are zero.
Now
$\Delta_{+R(x,y)}(R[][A, B_R]$ is the singleton relation $\{\tuple{A:x,B_R:y}\}$: the actual insertion.
The expression $\{\tuple{A:x,B_R:y}\} \bowtie S[][B_S, C] \bowtie B_R \theta B_S)$ can be simplified to
$\{\tuple{A:x}\} \bowtie S[][B_S, C] \bowtie y \theta B_S)$.



\subsubsection{Recursive incremental view maintenance \cite{koch-pods:10}.}


Before we come to multi-level incremental view maintenance, let us recap the special case of recursive incremental view maintenance of \cite{koch-pods:10, kennedy-ahmad-koch-cidr:11}, where we try to be as greedily incremental as possible.
If we restrict our query language to exclude aggregates nested into conditions (for which the delta query was complicated), the query language fragment has the following nice property \cite{koch-pods:10}:
For any query $Q$ of the fragment, $\Delta Q$ is again a query of the fragment. Moreover,
$\Delta Q$ is structurally strictly simpler than $Q$ when query complexity is measured as follows. For union-free queries, the complexity, called {\em join-height} is the number of relations joined together. We can use distributivity to push unions above joins and so give a complexity measure to queries with unions. 

Recursive incremental view maintenance makes use of the simple fact that a delta query is a query too. Thus it can be incrementally maintained as well, making use of a delta query to the delta query, which again can be materialized and incrementally maintained, and so on, recursively.
Since delta queries are structurally simpler than the base queries, this recursive query transformation terminates, which happens when the join-height of a $k$-th delta query comes down to zero (i.e., it does not contain any database relation). This happens for $k$ equal the join-height of the input query.

The goal of compilation is to create on-insert and on-delete triggers for every relation occurring in the query.
Conceptually, a query $Q[\vec{x}_{in}][\vec{x}{out}]$ of join-height $H$ over relations $R_1, \dots, R_k$ is compiled as follows:
We write $M_Q$ for the materialized view of query $Q$. 
Then the trigger program for the event $\pm R_{i_{j+1}}(\vec{y}_{j+1})$ consists of the
statements

\noindent
foreach $\vec{x}_{out}, \vec{y}_1, \dots, \vec{y}_j$ do \\
~~~
   $M_{\Delta_{\pm R_{i_j}} \dots \Delta_{\pm R_{i_1}} Q}
       [\vec{x}_{in}\vec{y}_1 \dots \vec{y}_j][\vec{x}_{out}] \;\pm= \\
~~~~~~
    M_{\Delta_{\pm R_{i_{j+1}}} \dots \Delta_{\pm R_{i_1}} Q}
       [\vec{x}_{in}\vec{y}_1 \dots \vec{y}_{j+1}][\vec{x}_{out}]$.


\noindent
for each $j \in 1 \dots H$ and $\tuple{i_1 \dots i_j} \in {1 \dots k}^j$. Note that for correctness, unless we maintain
old and new versions of the maps (which would be costly), these statements have to be 
ordered by increasing $j$. TODO: explain.

We observe that the structure of the work that needs to be done is extremely regular and (conceptually) simple. Moreover,
there are no classical large-granularity operators left, so it does not make sense to give this workload to a classical
query optimizer. There are for-loops over many variables, which have the potential to be very expensive. But the work
is also perfectly data parallel, and there are no data dependencies comparable to those present in joins. All this provides
justification for adopting a compilers approach. The workload has to be further decomposed and simplified, otherwise the
materializations will be of high dimensionality, very large, and so expensive to maintain. But if this decomposition
is done well, there is a chance to arrive at very efficient low-level code.



\subsubsection{Initial value computation}


In general, expressions with binding patterns have to be materialized, which causes difficulties: how to determine a suitable domain for these input variables for which to materialize the results of the expressions, how to represent and store such materialized structures, and how to dynamically maintain the domains of input variables as updates add previously unseen data values.

Maintaining the right domains for the variables is a challenge, particularly for input variables. Great optimization potential and so far we only have a na‹ve solution. Assuming all the relevant variables are in the domain, it?s easy, but that is an unrealistic assumption.


Theorem: case where values are zero.

Compile initial value computations for incremental evaluation. This needs non-query code.




\subsection{Join graph decomposition and factorization}
\label{sec:simplification}


In this subsection we present two query simplification techniques that are key to making
recursive incremental view maintenance useful, join graph decomposition, and factorization
of query polynomials.



\begin{verbatim}
R(AB), S(BC), T(CDE)

select sum(A*D) from natjoin(R,S,T) [][T.E]


+S(b,c):
   foreach x: q[][x] += select sum(A*D) from R,T [][E]
         = select sum(A) from R where B=b [][] *
            select sum(D) from T where C=c [][T.E/x]
\end{verbatim}


Factorization:





